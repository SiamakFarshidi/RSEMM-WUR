#!/usr/bin/env python3
"""
workflow2_full.py

A merged and optimized script that combines and extends all functionality from the original
workflow.py and Workflow2.py. This script performs the following analyses for each Zenodo JSON
file in a given directory:

1. FAIRness Assessment
   a. Local metadata-based sub-principle assessment (FAIR4RS sub-principles)
   b. Estimation of principle-level FAIR categories and overall FAIRness
   c. External "HowFairIs" GitHub-based checks for repository-level FAIR compliance

2. Software Engineering (SE) Practices Analysis
   a. Commit statistics: total commits, first/last commit dates, project duration
   b. Issue statistics: total issues, issue creation/closing rates
   c. Source code metrics: SLOC (source lines of code), CLOC (comment lines), test SLOC
   d. Repository extraction and cleanup

3. Code-Generation Detection
   a. Heuristic detection via commit-message keywords
   b. Notebook (.ipynb) file counting
   c. OpenAI-based classification to detect AI-assisted code generation

4. AI/ML/Ops Detection
   a. Keyword-based detection for AI/ML/Ops relevance in README or repository metadata
   b. OpenAI-based classification for ambiguous or missing signals

The script iterates over all Zenodo JSON files in the specified input directory, extracts the GitHub
URL from each, and runs the full suite of analyses. Results are merged back into the JSON under
"analysis_results" and written back. Processed filenames are tracked in a progress file to allow
incremental runs.

This version ensures that **all** functionality from the original workflow.py is preserved (including
local metadata-based FAIR assessment) and integrates optimizations for reuse, caching, and clarity.

Usage:
    python3 workflow2_full.py /path/to/zenodo_json_directory

Requirements:
    - Python 3.8+
    - Modules: requests, openai, python-dateutil

Environment Configuration:
    - Set the environment variable GITHUB_TOKEN to a personal access token (PAT) with repo access.
      If not set, GitHub API calls will be unauthenticated and may be rate-limited heavily.
    - Set the environment variable OPENAI_API_KEY to a valid OpenAI API key for LLM-based analyses.

Author:
    Generated by ChatGPT (OpenAI o4-mini), merged from workflow.py and Workflow2.py, optimized.
"""

import os
import sys
import json
import time
import re
import shutil
import tempfile
import requests
import logging
from datetime import datetime
from dateutil import parser as date_parser
import openai

# ─────────────────────────────────────────────────────────────────────────────
# CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────────

# Directory containing all Zenodo JSON files is provided as a command-line argument
if len(sys.argv) != 2:
    print("Usage: python3 workflow2_full.py /path/to/zenodo_json_directory", file=sys.stderr)
    sys.exit(1)

INPUT_DIR = sys.argv[1]
PROGRESS_FILE = "processed_files.json"

# OpenAI API configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
if not OPENAI_API_KEY:
    print("Warning: OPENAI_API_KEY is not set. OpenAI-based analyses will fail.", file=sys.stderr)
openai.api_key = OPENAI_API_KEY
OPENAI_MAX_RETRIES = 5
OPENAI_BACKOFF_BASE = 5  # seconds, doubles each retry

# GitHub API configuration
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "").strip()
if not GITHUB_TOKEN:
    print("Warning: GITHUB_TOKEN is not set. Unauthenticated GitHub API calls may be severely rate-limited.", file=sys.stderr)
GITHUB_DEFAULT_SLEEP = 5   # seconds to wait on generic GitHub errors
NETWORK_SLEEP = 30         # seconds to wait on network errors

# Caching controls (in-memory for this run; can be extended to on-disk if desired)
_CACHE = {
    "commit_stats": {},       # key: (owner, repo) -> commit stats dict
    "issue_metrics": {},      # key: (owner, repo) -> issue metrics dict
    "readme_text": {},        # key: (owner, repo) -> README string
    "file_tree": {}           # key: (owner, repo) -> list of file paths
}

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)


# ─────────────────────────────────────────────────────────────────────────────
# SECTION 1: HELPER FUNCTIONS FOR GITHUB API REQUESTS WITH RATE-LIMIT HANDLING
# ─────────────────────────────────────────────────────────────────────────────

def get_github_headers():
    """
    Return HTTP headers for GitHub API calls. If GITHUB_TOKEN is set, include an Authorization header.
    """
    headers = {
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "workflow2_full-script"
    }
    if GITHUB_TOKEN:
        headers["Authorization"] = f"token {GITHUB_TOKEN}"
    return headers


def wait_for_github_rate_limit(response_headers):
    """
    If the GitHub rate-limit is reached (HTTP 403 + X-RateLimit-Remaining: 0),
    sleep until reset time. Otherwise, do nothing.
    """
    try:
        remaining = int(response_headers.get("X-RateLimit-Remaining", "1"))
        if remaining == 0:
            reset_timestamp = int(response_headers.get("X-RateLimit-Reset", "0"))
            current_timestamp = time.time()
            wait_seconds = max(reset_timestamp - current_timestamp, GITHUB_DEFAULT_SLEEP)
            logger.warning(f"GitHub rate limit reached. Sleeping for {wait_seconds:.0f} seconds.")
            time.sleep(wait_seconds + 1)
    except Exception:
        # If parsing fails, just sleep a default amount
        logger.warning(f"Could not parse GitHub rate-limit headers. Sleeping for {GITHUB_DEFAULT_SLEEP}s.")
        time.sleep(GITHUB_DEFAULT_SLEEP)


def robust_github_request(method, url, params=None, retry_on_error=True):
    """
    Perform a GitHub API request (GET/POST/HEAD/etc.) with basic error handling and rate-limit handling.
    Retries on network errors and 5xx responses with exponential backoff.
    """
    backoff = 1
    while True:
        try:
            response = requests.request(method, url, headers=get_github_headers(), params=params, timeout=10)
        except requests.RequestException as e:
            logger.error(f"Network error during GitHub request to {url}: {e}. Retrying in {NETWORK_SLEEP}s.")
            time.sleep(NETWORK_SLEEP)
            continue

        if response.status_code == 403:  # Possibly rate-limited
            wait_for_github_rate_limit(response.headers)
            continue
        elif response.status_code in {500, 502, 503, 504} and retry_on_error:
            logger.warning(f"GitHub server error {response.status_code} at {url}. Retrying in {backoff}s.")
            time.sleep(backoff)
            backoff = min(backoff * 2, 60)
            continue
        else:
            return response


# ─────────────────────────────────────────────────────────────────────────────
# SECTION 2: HELPER FUNCTIONS FOR OPENAI CHAT COMPLETIONS WITH RATE-LIMIT HANDLING
# ─────────────────────────────────────────────────────────────────────────────

def call_openai_chat_completion(messages, model="gpt-3.5-turbo", max_tokens=400, temperature=0.0):
    """
    Wrapper around openai.ChatCompletion.create to handle rate-limit retries.
    messages: list of {"role": "system"/"user"/"assistant", "content": "..."} dicts.
    Returns the ChatCompletion response object on success.
    """
    retries = 0
    backoff = OPENAI_BACKOFF_BASE
    while True:
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            return response
        except openai.error.RateLimitError as e:
            if retries < OPENAI_MAX_RETRIES:
                wait = backoff * (2 ** retries)
                logger.warning(f"OpenAI rate limit error: {e}. Retrying in {wait}s.")
                time.sleep(wait)
                retries += 1
                continue
            else:
                logger.error(f"OpenAI rate limit error after {retries} retries: {e}.")
                raise
        except openai.error.APIError as e:
            # Retry on server errors
            if retries < OPENAI_MAX_RETRIES:
                wait = backoff * (2 ** retries)
                logger.warning(f"OpenAI API error {e}. Retrying in {wait}s.")
                time.sleep(wait)
                retries += 1
                continue
            else:
                logger.error(f"OpenAI API error after {retries} retries: {e}.")
                raise
        except Exception as e:
            logger.error(f"Unexpected error during OpenAI call: {e}.")
            raise


# ─────────────────────────────────────────────────────────────────────────────
# SECTION 3: FAIRNESS ASSESSMENT (LOCAL METADATA + EXTERNAL HowFairIs CHECKS)
# ─────────────────────────────────────────────────────────────────────────────

# 3.1 Local Metadata-Based Sub-Principle Assessment (FAIR4RS)

def score_value(tag):
    """
    Convert a FAIR sub-principle tag ("yes"/"no"/"partial"/"unknown"/"not applicable")
    into a numeric score: yes=1, partial=0.5, no=0, unknown/not applicable=0.
    """
    if tag == "yes":
        return 1.0
    elif tag == "partial":
        return 0.5
    else:
        return 0.0


def categorize_principle(numeric_scores):
    """
    Given a list of numeric scores for all sub-principles under a single FAIR principle,
    return "High"/"Medium"/"Low" based on average:
      - High: average >= 0.75
      - Medium: average >= 0.5
      - Low: otherwise
    """
    if not numeric_scores:
        return "Unknown"
    avg = sum(numeric_scores) / len(numeric_scores)
    if avg >= 0.75:
        return "High"
    elif avg >= 0.5:
        return "Medium"
    else:
        return "Low"


def estimate_fairness_from_subprinciples(sub: dict) -> dict:
    """
    From sub-principle results mapping (e.g., {"doi_present": "yes", "version_identifiers": "no", ...}),
    compute:
      {
        "principle_categories": {
            "Findable": "High"/"Medium"/"Low",
            "Accessible": "...",
            "Interoperable": "...",
            "Reusable": "..."
        },
        "overall_fairness": "High"/"Medium"/"Low"
      }
    Groups definitions derived from FAIR4RS specification.
    """
    groups = {
        "Findable": [
            "doi_present",
            "subcomponent_identifiers",
            "version_identifiers",
            "rich_metadata_present",
            "metadata_includes_doi"
        ],
        "Accessible": [
            "data_retrievable_protocol",
            "metadata_available",
            "metadata_accessible_standard",
            "metadata_authorization_clear"
        ],
        "Interoperable": [
            "license_machine_readable",
            "metadata_formats_standard",
            "metadata_vocabulary"
        ],
        "Reusable": [
            "license_kept",
            "metadata_provenance",
            "metadata_detail_reuse",
            "community_standards"
        ]
    }

    principle_categories = {}
    principle_scores = []

    for principle, keys in groups.items():
        numeric_scores = [score_value(sub.get(k, "unknown")) for k in keys]
        category = categorize_principle(numeric_scores)
        principle_categories[principle] = category
        avg_val = sum(numeric_scores) / len(numeric_scores) if numeric_scores else 0.0
        principle_scores.append(avg_val)

    overall_avg = sum(principle_scores) / len(principle_scores) if principle_scores else 0.0
    if overall_avg >= 0.75:
        overall = "High"
    elif overall_avg >= 0.5:
        overall = "Medium"
    else:
        overall = "Low"

    return {
        "principle_categories": principle_categories,
        "overall_fairness": overall
    }


def assess_fairness_metadata(record: dict) -> dict:
    """
    Return a dict mapping each FAIR4RS sub-principle to one of
    {"yes", "no", "partial", "unknown", "not applicable"} based on Zenodo metadata.
    Sub-principles considered:
      F1, F1.1, F1.2, F2, F3, F4,
      A1, A1.2, A2,
      I1, I2,
      R1.1, R1.2, R2, R3.

    The Zenodo record is assumed to follow the standard JSON schema from Zenodo.
    """
    results = {}
    metadata = record.get("metadata", {})

    # F1: DOI present?
    results["doi_present"] = "yes" if metadata.get("doi") else "no"

    # F1.1: Subcomponent identifiers (not typically in Zenodo metadata) => unknown
    results["subcomponent_identifiers"] = "unknown"

    # F1.2: Version identifiers? Look for metadata["relations"]["version"]
    versions = metadata.get("relations", {}).get("version", [])
    results["version_identifiers"] = "yes" if versions else "no"

    # F2: Rich metadata present? Check for abstract and detailed description
    if metadata.get("title") and metadata.get("description"):
        results["rich_metadata_present"] = "yes"
    else:
        results["rich_metadata_present"] = "no"

    # F3: Metadata includes DOI? (for underlying datasets, etc.)
    # Check if metadata "related_identifiers" contains another DOI
    rel_ids = metadata.get("related_identifiers", [])
    if any("doi" in (ri.get("scheme") or "").lower() for ri in rel_ids):
        results["metadata_includes_doi"] = "yes"
    else:
        results["metadata_includes_doi"] = "no"

    # F4: Metadata accessible? Zenodo JSON is accessible via HTTP => yes
    results["metadata_available"] = "yes"

    # A1: Data retrievable via standard protocol? Zenodo API uses HTTP => yes
    results["data_retrievable_protocol"] = "yes"

    # A1.2: Authentication / authorization clear? If metadata has access_right => if "open-access" then yes
    access = metadata.get("access_right")
    if access and access.lower() == "open-access":
        results["metadata_authorization_clear"] = "yes"
    else:
        results["metadata_authorization_clear"] = "no"

    # A2: Metadata remains accessible even if data removed? Hard to know => unknown
    results["metadata_accessible_standard"] = "unknown"

    # I1: License machine-readable? Check if 'license' field is present
    license_info = metadata.get("license")
    results["license_machine_readable"] = "yes" if license_info else "no"

    # I2: Metadata formats follow community standards? Zenodo uses schema.org JSON-LD => yes
    results["metadata_formats_standard"] = "yes"

    # For I2: metadata_vocabulary? Assume yes since Zenodo uses standard vocabularies
    results["metadata_vocabulary"] = "yes"

    # R1.1: License kept? (license field exists) => same as I1
    results["license_kept"] = "yes" if license_info else "no"

    # R1.2: Metadata provenance? Hard to check; Zenodo records have creators => yes
    results["metadata_provenance"] = "yes" if metadata.get("creators") else "no"

    # R2: Community standards for critical data? Hard to know => unknown
    results["community_standards"] = "unknown"

    # R3: Detailed metadata for reuse? Check if keywords/tags exist
    keywords = metadata.get("keywords", [])
    results["metadata_detail_reuse"] = "yes" if keywords else "no"

    return results


# 3.2 External "HowFairIs" Checks (GitHub-Based)

def extract_owner_repo_from_url(repo_url):
    """
    Given a GitHub repository URL (e.g., https://github.com/user/repo or git@github.com:user/repo.git),
    extract and return (owner, repo) tuple. Returns (None, None) if parsing fails.
    """
    if not repo_url:
        return None, None

    # Normalize SSH URLs to HTTPS form
    if repo_url.startswith("git@github.com:"):
        repo_part = repo_url.replace("git@github.com:", "").rstrip(".git")
        parts = repo_part.split("/")
        if len(parts) == 2:
            return parts[0], parts[1]
        else:
            return None, None
    # Normalize HTTPS URLs
    try:
        # Remove .git suffix if present
        if repo_url.endswith(".git"):
            repo_url = repo_url[:-4]
        pattern = r"github\.com[:/]+([^/]+)/([^/]+)"
        m = re.search(pattern, repo_url)
        if m:
            owner = m.group(1)
            repo = m.group(2)
            return owner, repo
    except Exception:
        pass

    return None, None


def fetch_readme_text(owner, repo):
    """
    Fetch the README.md (or README) text content from the default branch of a GitHub repository.
    Caches results in-memory for the duration of the script.
    Returns README as a string, or "" if none found.
    """
    key = (owner, repo)
    if key in _CACHE["readme_text"]:
        return _CACHE["readme_text"][key]

    # Try common README filenames
    for filename in ["README.md", "README.MD", "README", "readme.md", "readme.MD", "readme"]:
        url = f"https://raw.githubusercontent.com/{owner}/{repo}/master/{filename}"
        response = robust_github_request("GET", url, retry_on_error=False)
        if response.status_code == 200:
            text = response.text
            _CACHE["readme_text"][key] = text
            return text
    # If not on master, try default branch via API
    api_url = f"https://api.github.com/repos/{owner}/{repo}/readme"
    response = robust_github_request("GET", api_url)
    if response.status_code == 200:
        json_data = response.json()
        if "content" in json_data and json_data.get("encoding") == "base64":
            import base64
            content = base64.b64decode(json_data["content"]).decode("utf-8", errors="ignore")
            _CACHE["readme_text"][key] = content
            return content
    _CACHE["readme_text"][key] = ""
    return ""


def check_registry_badge_in_readme(readme_text):
    """
    Return True if the README text contains a Zenodo (registry) badge,
    e.g., something like [![DOI](https://zenodo.org/badge/DOI/...)] or similar patterns.
    """
    if not readme_text:
        return False
    # Simplistic check for "zenodo.org/badge/DOI"
    return bool(re.search(r"zenodo\.org/badge/DOI", readme_text, re.IGNORECASE))


def check_quality_badge_in_readme(readme_text):
    """
    Return True if the README text contains any common CI/quality badge,
    e.g., Travis CI, GitHub Actions, CircleCI, etc.
    """
    if not readme_text:
        return False
    patterns = [
        r"\[!\[Build Status\]\(",    # Typically Travis CI or others
        r"shields\.io",             # Shields.io badges
        r"circleci\.com",
        r"github\.com/.+/actions/workflows",
        r"travis-ci\.org",
        r"appveyor\.com",
        r"coveralls\.io",
        r"codecov\.io"
    ]
    for pat in patterns:
        if re.search(pat, readme_text, re.IGNORECASE):
            return True
    return False


def run_howfairis_checks(repo_url: str) -> dict:
    """
    Perform 5 checks via GitHub API / README parsing:
      1) Repository exists?
      2) License exists?
      3) Registry badge (Zenodo) in README?
      4) CITATION.cff or CITATION.md exists?
      5) Quality badge in README?

    Returns:
      {
        "howfairis_score": <0–5 int>,
        "howfairis_details": {
            "repository": "yes"/"no",
            "license": "yes"/"no",
            "registry": "yes"/"no",
            "citation": "yes"/"no",
            "quality": "yes"/"no"
        }
      }
    """
    details = {
        "repository": "no",
        "license": "no",
        "registry": "no",
        "citation": "no",
        "quality": "no"
    }
    score = 0

    owner, repo = extract_owner_repo_from_url(repo_url)
    if not owner or not repo:
        return {
            "howfairis_score": score,
            "howfairis_details": details
        }

    # 1) Check repository exists (public)
    repo_api = f"https://api.github.com/repos/{owner}/{repo}"
    response = robust_github_request("GET", repo_api)
    if response.status_code == 200:
        details["repository"] = "yes"
        score += 1
    else:
        # Repository missing or private
        return {
            "howfairis_score": score,
            "howfairis_details": details
        }

    # 2) License exists: via /license endpoint
    license_api = f"https://api.github.com/repos/{owner}/{repo}/license"
    li_response = robust_github_request("GET", license_api)
    if li_response.status_code == 200:
        details["license"] = "yes"
        score += 1

    # 3) & 5) Fetch README text once
    readme_text = fetch_readme_text(owner, repo)

    # 3) Registry badge?
    if check_registry_badge_in_readme(readme_text):
        details["registry"] = "yes"
        score += 1

    # 4) CITATION.cff or CITATION.md exists?
    for filename in ["CITATION.cff", "CITATION.md", "citation.cff", "citation.md"]:
        raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/master/{filename}"
        cit_response = robust_github_request("GET", raw_url, retry_on_error=False)
        if cit_response.status_code == 200:
            details["citation"] = "yes"
            score += 1
            break

    # 5) Quality badge?
    if check_quality_badge_in_readme(readme_text):
        details["quality"] = "yes"
        score += 1

    return {
        "howfairis_score": score,
        "howfairis_details": details
    }


# ─────────────────────────────────────────────────────────────────────────────
# SECTION 4: SOFTWARE ENGINEERING (SE) PRACTICES ANALYSIS
# ─────────────────────────────────────────────────────────────────────────────

# 4.1 GitHub Commit Statistics

def fetch_all_commits(owner, repo):
    """
    Fetch all commits for a given repository (owner, repo) via the GitHub API,
    returning a list of commit metadata dicts. Caches results in-memory for this run.
    Implements pagination and rate-limit handling.
    """
    key = (owner, repo)
    if key in _CACHE["commit_stats"]:
        return _CACHE["commit_stats"][key]["all_commits"]

    commits = []
    per_page = 100
    page = 1

    while True:
        url = f"https://api.github.com/repos/{owner}/{repo}/commits"
        params = {"per_page": per_page, "page": page}
        response = robust_github_request("GET", url, params=params)
        if response.status_code != 200:
            logger.warning(f"Failed to fetch commits for {owner}/{repo}, status {response.status_code}.")
            break
        data = response.json()
        if not data:
            break
        commits.extend(data)
        if len(data) < per_page:
            break
        page += 1

    # Cache raw commit list; details like total count and dates will be computed later
    _CACHE["commit_stats"][key] = {"all_commits": commits}
    return commits


def compute_commit_stats(owner, repo):
    """
    Given owner and repo, compute:
      - total_commits (int)
      - first_commit_date (ISO string)
      - last_commit_date (ISO string)
      - project_duration_days (float)
    Returns a dict with those keys.
    """
    commits = fetch_all_commits(owner, repo)
    total_commits = len(commits)
    commit_dates = []
    for c in commits:
        # Use commit.commit.author.date if available, else skip
        try:
            date_str = c["commit"]["author"]["date"]
            dt = date_parser.parse(date_str)
            commit_dates.append(dt)
        except Exception:
            continue

    if commit_dates:
        first = min(commit_dates)
        last = max(commit_dates)
        duration = (last - first).total_seconds() / (3600 * 24)  # days
        first_iso = first.isoformat()
        last_iso = last.isoformat()
    else:
        first_iso = None
        last_iso = None
        duration = 0.0

    stats = {
        "total_commits": total_commits,
        "first_commit_date": first_iso,
        "last_commit_date": last_iso,
        "project_duration_days": duration
    }
    # Cache more detailed stats
    key = (owner, repo)
    _CACHE["commit_stats"][key].update(stats)
    return stats


# 4.2 GitHub Issue Metrics

def fetch_issue_metrics(owner, repo):
    """
    Fetch issue metrics for the specified repository.
    Returns a dict containing:
      - total_issues: total number of issues (open + closed)
      - open_issues_count: number of currently open issues (from repo metadata)
      - closed_issues_count: computed from events (closed issues only)
      - issue_events: list of issue events (each event as dict)
    Implements pagination and rate-limit handling. Caches results in-memory.
    """
    key = (owner, repo)
    if key in _CACHE["issue_metrics"]:
        return _CACHE["issue_metrics"][key]

    issue_events = []
    per_page = 100
    page = 1

    # Use the issues API; include both open and closed issues
    while True:
        url = f"https://api.github.com/repos/{owner}/{repo}/issues"
        params = {"state": "all", "per_page": per_page, "page": page}
        response = robust_github_request("GET", url, params=params)
        if response.status_code != 200:
            logger.warning(f"Failed to fetch issues for {owner}/{repo}, status {response.status_code}.")
            break
        data = response.json()
        if not data:
            break
        issue_events.extend(data)
        if len(data) < per_page:
            break
        page += 1

    # Compute metrics
    total_issues = len(issue_events)
    open_issues_count = sum(1 for issue in issue_events if issue.get("state") == "open")
    closed_issues_count = total_issues - open_issues_count

    metrics = {
        "total_issues": total_issues,
        "open_issues_count": open_issues_count,
        "closed_issues_count": closed_issues_count,
        "issue_events": issue_events
    }
    _CACHE["issue_metrics"][key] = metrics
    return metrics


# 4.3 Repository Download & Extraction

def download_and_extract_repo(owner, repo, tmp_root):
    """
    Download the GitHub repository as a ZIP archive from the default branch (usually master/main)
    and extract it into a temporary directory under tmp_root. Returns the local path to the extracted
    repository root. If download or extraction fails, returns None.
    """
    # Determine default branch via API to handle main/master rename
    repo_api = f"https://api.github.com/repos/{owner}/{repo}"
    resp = robust_github_request("GET", repo_api)
    if resp.status_code != 200:
        logger.warning(f"Could not fetch repo metadata for {owner}/{repo}. Skipping download.")
        return None
    repo_data = resp.json()
    default_branch = repo_data.get("default_branch", "master")

    # Download ZIP archive
    zip_url = f"https://github.com/{owner}/{repo}/archive/refs/heads/{default_branch}.zip"
    response = robust_github_request("GET", zip_url, retry_on_error=False)
    if response.status_code != 200:
        logger.warning(f"Failed to download ZIP for {owner}/{repo}, status {response.status_code}.")
        return None

    # Write ZIP to a temp file
    zip_path = os.path.join(tmp_root, f"{owner}_{repo}.zip")
    try:
        with open(zip_path, "wb") as f:
            f.write(response.content)
    except Exception as e:
        logger.error(f"Error writing ZIP file for {owner}/{repo}: {e}")
        return None

    # Extract the ZIP
    extract_path = os.path.join(tmp_root, f"{owner}_{repo}_extracted")
    try:
        shutil.unpack_archive(zip_path, extract_path)
    except Exception as e:
        logger.error(f"Error extracting ZIP for {owner}/{repo}: {e}")
        return None

    # After extraction, the files will be under a subdirectory like "{repo}-{default_branch}"
    # Find that directory
    for entry in os.listdir(extract_path):
        candidate = os.path.join(extract_path, entry)
        if os.path.isdir(candidate) and entry.startswith(repo):
            return candidate

    # Fallback: return extract_path itself
    return extract_path


# 4.4 Source File Analysis (SLOC, CLOC, Test SLOC)

def analyze_source_file(file_path):
    """
    Read a source file and count:
      - sloc: number of non-blank, non-comment lines (approximate)
      - cloc: number of comment lines (approximate)
      - test_sloc: if the file path indicates a test (e.g., /tests/ or test_*.py), count its sloc separately
    Supports .py, .R, .ipynb (counts code cells), .java, .js, .cpp, .c, .h, .m, .r, .jl, .go, .rs, etc.
    Returns a tuple (sloc, cloc, test_sloc).
    """
    sloc = 0
    cloc = 0
    test_sloc = 0

    # Determine if this is a test file by filename or path
    is_test_file = bool(re.search(r"(/tests?/|(^|_)test_)", file_path, re.IGNORECASE))

    # File extension
    ext = os.path.splitext(file_path)[1].lower()

    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()
    except Exception:
        return 0, 0, 0

    if ext == ".ipynb":
        # For Jupyter notebooks, count code cells
        try:
            import nbformat
            nb = nbformat.reads("".join(lines), as_version=4)
            for cell in nb.cells:
                if cell.cell_type == "code":
                    for cl in cell.source.splitlines():
                        if cl.strip().startswith("#"):
                            cloc += 1
                        elif cl.strip():
                            if is_test_file:
                                test_sloc += 1
                            else:
                                sloc += 1
        except Exception:
            # Fallback: treat as zero
            pass
        return sloc, cloc, test_sloc

    # Single-line comment markers by language
    comment_markers = {
        ".py": "#",
        ".r": "#",
        ".jl": "#",
        ".m": "%",       # MATLAB / Octave
        ".c": "//",
        ".cpp": "//",
        ".h": "//",
        ".java": "//",
        ".js": "//",
        ".go": "//",
        ".rs": "//",
        ".swift": "//",
        ".kt": "//",
        ".ts": "//",
        ".sh": "#"
    }
    marker = comment_markers.get(ext, None)

    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        if marker and stripped.startswith(marker):
            cloc += 1
        elif ext in [".c", ".cpp", ".h", ".java", ".js", ".go", ".rs", ".kt", ".ts"]:
            # Check for block comments start/end (rough heuristic)
            if stripped.startswith("/*") or stripped.startswith("*") or stripped.startswith("*/"):
                cloc += 1
            else:
                if is_test_file:
                    test_sloc += 1
                else:
                    sloc += 1
        else:
            if is_test_file:
                test_sloc += 1
            else:
                sloc += 1

    return sloc, cloc, test_sloc


def run_se_practices_analysis(repo_url, tmp_root):
    """
    Perform the full SE practices analysis for a repository:
      - Commit stats (calls compute_commit_stats)
      - Issue metrics (calls fetch_issue_metrics)
      - Download & extract repo (calls download_and_extract_repo)
      - Walk through files under extracted repo root and call analyze_source_file()
      - Summarize total sloc, cloc, test_sloc counts
    Returns a dict with keys:
      {
         "commit_stats": { ... },
         "issue_metrics": { ... },
         "source_code_metrics": {
             "total_sloc": int,
             "total_cloc": int,
             "total_test_sloc": int,
             "file_count": int
         }
      }
    """
    se_results = {
        "commit_stats": {},
        "issue_metrics": {},
        "source_code_metrics": {
            "total_sloc": 0,
            "total_cloc": 0,
            "total_test_sloc": 0,
            "file_count": 0
        }
    }

    owner, repo = extract_owner_repo_from_url(repo_url)
    if not owner or not repo:
        logger.warning(f"Could not parse owner/repo from URL: {repo_url}. Skipping SE analysis.")
        return se_results

    # -- Commit Stats --
    commit_stats = compute_commit_stats(owner, repo)
    se_results["commit_stats"] = commit_stats

    # -- Issue Metrics --
    issue_metrics = fetch_issue_metrics(owner, repo)
    # Remove the full list of issue_events from output for brevity; keep aggregate stats
    aggregated_issue_metrics = {
        "total_issues": issue_metrics["total_issues"],
        "open_issues_count": issue_metrics["open_issues_count"],
        "closed_issues_count": issue_metrics["closed_issues_count"]
    }
    se_results["issue_metrics"] = aggregated_issue_metrics

    # -- Source Code Metrics --
    repo_root = download_and_extract_repo(owner, repo, tmp_root)
    if repo_root:
        total_sloc = 0
        total_cloc = 0
        total_test_sloc = 0
        file_count = 0

        for root, dirs, files in os.walk(repo_root):
            for fname in files:
                fpath = os.path.join(root, fname)
                ext = os.path.splitext(fpath)[1].lower()
                # Consider common source/code file extensions
                if ext in {
                    ".py", ".r", ".jl", ".m", ".c", ".cpp", ".h", ".java", ".js",
                    ".go", ".rs", ".kt", ".ts", ".ipynb", ".swift", ".sh"
                }:
                    sloc, cloc, test_sloc = analyze_source_file(fpath)
                    total_sloc += sloc
                    total_cloc += cloc
                    total_test_sloc += test_sloc
                    file_count += 1

        se_results["source_code_metrics"] = {
            "total_sloc": total_sloc,
            "total_cloc": total_cloc,
            "total_test_sloc": total_test_sloc,
            "file_count": file_count
        }

    # Cleanup extracted repository if present
    try:
        if repo_root and os.path.exists(repo_root):
            shutil.rmtree(os.path.dirname(repo_root), ignore_errors=True)
    except Exception as e:
        logger.warning(f"Error cleaning up temp directory for {owner}/{repo}: {e}")

    return se_results


# ─────────────────────────────────────────────────────────────────────────────
# SECTION 5: CODE-GENERATION DETECTION
# ─────────────────────────────────────────────────────────────────────────────

CODEGEN_KEYWORDS = {
    "copilot", "co-pilot", "co pilot", "co_pilot", "github copilot", "git copilot",
    "chatgpt", "chat gpt", "gpt4", "gpt-4", "gpt 4", "openai", "generative ai", "ai-generated"
}


def detect_commits_with_code_generation_keywords(commit_messages):
    """
    Check commit messages for any mention of common code-generation keywords.
    Returns True if any keyword found; else False.
    """
    for msg in commit_messages:
        text = msg.lower()
        for kw in CODEGEN_KEYWORDS:
            if kw in text:
                return True
    return False


def fetch_commit_messages(owner, repo):
    """
    Retrieve commit messages (strings) for the given repository.
    Uses cached commit list from fetch_all_commits.
    """
    commits = fetch_all_commits(owner, repo)
    messages = []
    for c in commits:
        try:
            msg = c["commit"]["message"]
            messages.append(msg)
        except Exception:
            continue
    return messages


def count_notebook_files(owner, repo):
    """
    Count the number of .ipynb files in the repository by traversing the file tree via the GitHub API.
    Returns an integer. Caches results in-memory.
    """
    key = (owner, repo)
    if key in _CACHE["file_tree"]:
        file_list = _CACHE["file_tree"][key]
    else:
        # Use the Git Trees API to fetch all file paths recursively
        # First, get default branch
        repo_api = f"https://api.github.com/repos/{owner}/{repo}"
        resp = robust_github_request("GET", repo_api)
        if resp.status_code != 200:
            return 0
        default_branch = resp.json().get("default_branch", "master")
        # Get tree SHA for default branch
        ref_api = f"https://api.github.com/repos/{owner}/{repo}/git/refs/heads/{default_branch}"
        ref_resp = robust_github_request("GET", ref_api)
        if ref_resp.status_code != 200:
            return 0
        sha = ref_resp.json().get("object", {}).get("sha")
        if not sha:
            return 0
        # Get the full tree recursively
        tree_api = f"https://api.github.com/repos/{owner}/{repo}/git/trees/{sha}"
        params = {"recursive": 1}
        tree_resp = robust_github_request("GET", tree_api, params=params)
        if tree_resp.status_code != 200:
            return 0
        tree_data = tree_resp.json().get("tree", [])
        file_list = [entry["path"] for entry in tree_data if entry["type"] == "blob"]
        _CACHE["file_tree"][key] = file_list

    # Count .ipynb files
    count = sum(1 for p in file_list if p.lower().endswith(".ipynb"))
    return count


def detect_codegen_with_openai(owner, repo, readme_excerpt, commit_messages):
    """
    Use OpenAI to classify whether the repository code is AI-assisted/generated.
    The prompt expects a JSON output: {"generated": "yes"/"no", "confidence": float}
    """
    system_msg = {
        "role": "system",
        "content": (
            "You are an AI assistant specialized in detecting AI-generated code in GitHub repositories. "
            "Given the following information (README excerpt and recent commit messages), respond with "
            "a JSON object containing two fields: 'generated' (string 'yes' or 'no') and 'confidence' (float between 0 and 1)."
        )
    }
    user_content = (
        f"README excerpt:\n{readme_excerpt}\n\n"
        f"Commit messages (most recent 100):\n" + "\n".join(commit_messages[-100:])
    )
    user_msg = {"role": "user", "content": user_content}

    try:
        response = call_openai_chat_completion([system_msg, user_msg], model="gpt-3.5-turbo", max_tokens=200)
        text = response.choices[0].message["content"].strip()
        # Attempt to parse JSON from the response
        data = json.loads(text)
        generated = data.get("generated", "no")
        confidence = float(data.get("confidence", 0.0))
    except Exception as e:
        logger.warning(f"OpenAI codegen detection failed for {owner}/{repo}: {e}")
        generated = "unknown"
        confidence = 0.0

    return {"generated": generated, "confidence": confidence}


def run_codegen_detection(repo_url):
    """
    Perform code generation detection for a given repository URL:
      1) Fetch commit messages
      2) Check for codegen keywords in commits
      3) Count notebook files
      4) Fetch README excerpt (first 2000 chars)
      5) Invoke OpenAI classifier
    Returns a dict with keys:
      {
        "commit_keyword_flag": True/False,
        "notebook_count": int,
        "llm_classification": {"generated": "yes"/"no"/"unknown", "confidence": float}
      }
    """
    owner, repo = extract_owner_repo_from_url(repo_url)
    if not owner or not repo:
        logger.warning(f"Could not parse owner/repo from URL: {repo_url}. Skipping codegen detection.")
        return {
            "commit_keyword_flag": False,
            "notebook_count": 0,
            "llm_classification": {"generated": "unknown", "confidence": 0.0}
        }

    # 1) Commit messages
    commit_messages = fetch_commit_messages(owner, repo)
    keyword_flag = detect_commits_with_code_generation_keywords(commit_messages)

    # 2) Notebook count
    nb_count = count_notebook_files(owner, repo)

    # 3) README excerpt
    readme_full = fetch_readme_text(owner, repo)
    readme_excerpt = readme_full[:2000] if readme_full else ""

    # 4) OpenAI classification
    llm_result = detect_codegen_with_openai(owner, repo, readme_excerpt, commit_messages)

    return {
        "commit_keyword_flag": keyword_flag,
        "notebook_count": nb_count,
        "llm_classification": llm_result
    }


# ─────────────────────────────────────────────────────────────────────────────
# SECTION 6: AI/ML/OPS DETECTION
# ─────────────────────────────────────────────────────────────────────────────

AIOPS_KEYWORDS = {
    "machine learning", "ml", "deep learning", "neural network", "ai", "artificial intelligence",
    "tensorflow", "pytorch", "keras", "sklearn", "scikit-learn", "mlflow", "serving", "onnx",
    "aiops", "mlo ps", "mlo_ps", "mlo-ps"
}

AIOPS_TYPES = [
    "computer vision", "nlp", "reinforcement learning", "data preprocessing", "model training",
    "model evaluation", "model deployment", "monitoring", "pipelines", "notebooks", "data wrangling"
]


def detect_aiops_with_openai(owner, repo, readme_excerpt):
    """
    Use OpenAI to classify whether the repository is AI/ML/Ops-related.
    The expected JSON output:
      {"broad": "yes"/"no", "types": ["...", "..."], "confidence": float}
    """
    system_msg = {
        "role": "system",
        "content": (
            "You are an AI assistant specialized in detecting AI/ML/Ops content in GitHub repositories. "
            "Given the README excerpt, determine whether the project is broadly AI/ML/Ops-related, "
            "and list any specific subtypes (e.g., 'computer vision', 'nlp') if applicable. "
            "Respond with a JSON object containing 'broad' ('yes' or 'no'), "
            "'types' (a list of strings), and 'confidence' (float between 0 and 1)."
        )
    }
    user_msg = {"role": "user", "content": f"README excerpt:\n{readme_excerpt}"}

    try:
        response = call_openai_chat_completion([system_msg, user_msg], model="gpt-3.5-turbo", max_tokens=200)
        text = response.choices[0].message["content"].strip()
        data = json.loads(text)
        broad = data.get("broad", "no")
        types = data.get("types", [])
        confidence = float(data.get("confidence", 0.0))
    except Exception as e:
        logger.warning(f"OpenAI AI/ML/Ops detection failed for {owner}/{repo}: {e}")
        broad = "unknown"
        types = []
        confidence = 0.0

    return {"broad": broad, "types": types, "confidence": confidence}


def detect_ai_ml_ops(repo_url):
    """
    Perform AI/ML/Ops detection for a repository:
      1) Check README + repository description for keyword matches.
      2) If keyword presence indicates 'yes', no need for LLM.
      3) Otherwise, if ambiguous or no keywords, invoke OpenAI classifier.
    Returns a dict:
      {
        "keyword_flag": True/False,
        "llm_result": {"broad": "...", "types": [...], "confidence": float}
      }
    """
    owner, repo = extract_owner_repo_from_url(repo_url)
    if not owner or not repo:
        logger.warning(f"Could not parse owner/repo from URL: {repo_url}. Skipping AI/ML/Ops detection.")
        return {
            "keyword_flag": False,
            "llm_result": {"broad": "unknown", "types": [], "confidence": 0.0}
        }

    # Fetch README excerpt and repository description via API
    readme_full = fetch_readme_text(owner, repo)
    readme_excerpt = readme_full[:2000] if readme_full else ""

    # Also fetch repo description via API if needed
    repo_api = f"https://api.github.com/repos/{owner}/{repo}"
    response = robust_github_request("GET", repo_api)
    desc = ""
    if response.status_code == 200:
        desc = response.json().get("description", "") or ""
    combined_text = f"{desc}\n{readme_excerpt}".lower()

    # Keyword scan
    keyword_flag = False
    for kw in AIOPS_KEYWORDS:
        if kw in combined_text:
            keyword_flag = True
            break

    if keyword_flag:
        # Derive subtypes by scanning for AIOPS_TYPES in text
        types_found = [t for t in AIOPS_TYPES if t.lower() in combined_text]
        return {
            "keyword_flag": True,
            "llm_result": {"broad": "yes", "types": types_found, "confidence": 1.0}
        }
    else:
        # Use LLM for classification
        llm = detect_aiops_with_openai(owner, repo, readme_excerpt)
        return {
            "keyword_flag": False,
            "llm_result": llm
        }


# ─────────────────────────────────────────────────────────────────────────────
# SECTION 7: UTILITY FUNCTIONS
# ─────────────────────────────────────────────────────────────────────────────

def extract_git_url_from_zenodo(record: dict) -> str:
    """
    Extract a GitHub URL from the Zenodo record metadata:
      - Look for record["metadata"]["related_identifiers"] entries with relation 'isSupplementTo'
        and scheme 'DOI' pointing to a GitHub DOI (rare).
      - Look for record["metadata"]["urls"] containing GitHub links.
      - Look for record["metadata"]["community"]["url"] if present.
      - Look for description or notes containing 'github.com'.
    Returns the first valid GitHub URL found, else returns None.
    """
    # Method 1: metadata["urls"]
    metadata = record.get("metadata", {})
    urls = metadata.get("urls", [])
    for u in urls:
        url = u.get("url") or ""
        if "github.com" in url:
            return url

    # Method 2: metadata["related_identifiers"]
    rel_ids = metadata.get("related_identifiers", [])
    for ri in rel_ids:
        url = ri.get("identifier") or ""
        if "github.com" in url:
            return url

    # Method 3: metadata["description"] / "notes"
    desc = metadata.get("description", "") or ""
    match = re.search(r"(https?://github\.com/[A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+)", desc)
    if match:
        return match.group(1)

    # No URL found
    return None


def load_or_initialize_progress(progress_file):
    """
    Load the JSON list of processed filenames from progress_file.
    If the file does not exist, create it with an empty list.
    Returns the set of processed filenames.
    """
    if os.path.exists(progress_file):
        try:
            with open(progress_file, "r", encoding="utf-8") as pf:
                data = json.load(pf)
            if isinstance(data, list):
                return set(data)
            else:
                return set()
        except Exception:
            return set()
    else:
        with open(progress_file, "w", encoding="utf-8") as pf:
            json.dump([], pf)
        return set()


def update_progress(progress_file, processed_set):
    """
    Write the updated list of processed filenames back to progress_file.
    """
    try:
        with open(progress_file, "w", encoding="utf-8") as pf:
            json.dump(list(processed_set), pf, indent=2)
    except Exception as e:
        logger.error(f"Failed to update progress file: {e}")


# ─────────────────────────────────────────────────────────────────────────────
# SECTION 8: MAIN LOOP
# ─────────────────────────────────────────────────────────────────────────────

def main():
    # 1) Load or initialize progress
    processed = load_or_initialize_progress(PROGRESS_FILE)

    # 2) List all Zenodo JSON files in INPUT_DIR
    try:
        all_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(".json")]
    except Exception as e:
        logger.error(f"Error listing files in directory {INPUT_DIR}: {e}")
        sys.exit(1)

    total_files = len(all_files)
    logger.info(f"Found {total_files} JSON files in {INPUT_DIR}.")

    # 3) Process files one by one
    count = 0
    for filename in all_files:
        count += 1
        if filename in processed:
            logger.info(f"[{count}/{total_files}] Skipping already processed file: {filename}")
            continue

        filepath = os.path.join(INPUT_DIR, filename)
        logger.info(f"[{count}/{total_files}] Processing file: {filename}")
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                record = json.load(f)
        except Exception as e:
            logger.error(f"Failed to load JSON from {filename}: {e}")
            continue

        # 4) Extract GitHub URL
        git_url = extract_git_url_from_zenodo(record)
        has_git_url = bool(git_url)
        if not has_git_url:
            logger.warning(f"No GitHub URL found in {filename}. Skipping GitHub-based analyses.")
            # Still perform local metadata-based FAIR assessment
        else:
            logger.info(f"Found GitHub URL: {git_url}")

        # 5) FAIRness Assessment
        # 5a) Local metadata-based sub-principle assessment
        fair_subp = assess_fairness_metadata(record)

        # 5b) Principle-level categorization & overall fairness
        fair_est = estimate_fairness_from_subprinciples(fair_subp)

        # 5c) External HowFairIs (GitHub) checks
        hf = {"howfairis_score": None, "howfairis_details": None}
        if has_git_url:
            hf = run_howfairis_checks(git_url)
        else:
            hf = {"howfairis_score": 0, "howfairis_details": {
                "repository": "no", "license": "no", "registry": "no", "citation": "no", "quality": "no"
            }}

        fairness_result = {
            "subprinciple_assessment": fair_subp,
            "principle_categories": fair_est["principle_categories"],
            "overall_fairness": fair_est["overall_fairness"],
            "git_url": git_url or None,
            "howfairis_score": hf["howfairis_score"],
            "howfairis_details": hf["howfairis_details"]
        }

        # 6) SE Practices Analysis
        se_results = {}
        if has_git_url:
            tmp_root = tempfile.mkdtemp(prefix="se_analysis_")
            se_results = run_se_practices_analysis(git_url, tmp_root)
        else:
            se_results = {
                "commit_stats": {
                    "total_commits": 0, "first_commit_date": None, "last_commit_date": None, "project_duration_days": 0.0
                },
                "issue_metrics": {
                    "total_issues": 0, "open_issues_count": 0, "closed_issues_count": 0
                },
                "source_code_metrics": {
                    "total_sloc": 0, "total_cloc": 0, "total_test_sloc": 0, "file_count": 0
                }
            }

        # 7) Code-Generation Detection
        codegen_results = {}
        if has_git_url:
            codegen_results = run_codegen_detection(git_url)
        else:
            codegen_results = {
                "commit_keyword_flag": False,
                "notebook_count": 0,
                "llm_classification": {"generated": "no", "confidence": 0.0}
            }

        # 8) AI/ML/Ops Detection
        ai_ml_ops_results = {}
        if has_git_url:
            ai_ml_ops_results = detect_ai_ml_ops(git_url)
        else:
            ai_ml_ops_results = {
                "keyword_flag": False,
                "llm_result": {"broad": "no", "types": [], "confidence": 0.0}
            }

        # 9) Aggregate all analysis results
        analysis_results = {
            "fairness_assessment": fairness_result,
            "se_practices": se_results,
            "code_generation_detection": codegen_results,
            "ai_ml_ops_detection": ai_ml_ops_results
        }

        # 10) Merge results back into record
        record["analysis_results"] = analysis_results

        # 11) Write updated JSON back to file (overwrite)
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(record, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to write updated JSON to {filename}: {e}")
            continue

        # 12) Update progress
        processed.add(filename)
        update_progress(PROGRESS_FILE, processed)
        logger.info(f"Finished processing {filename}. Progress saved.")

    logger.info("All files processed.")


if __name__ == "__main__":
    main()
